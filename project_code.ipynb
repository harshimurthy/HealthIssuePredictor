{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "%matplotlib inline\n",
    "from scipy.stats import mode\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import namedtuple, Counter, defaultdict\n",
    "from math import log,ceil,sqrt\n",
    "import cvxopt\n",
    "from cvxopt import solvers,matrix\n",
    "from sklearn import metrics\n",
    "from math import exp,log10\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(Y_actual,Y_predicted):\n",
    "    conf_matrix =  metrics.confusion_matrix(Y_actual,Y_predicted)\n",
    "    sns.heatmap(conf_matrix, annot=True,  fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####function to normalize the data##\n",
    "\n",
    "def norm(Z):\n",
    "    mean = np.sum(Z,axis=0)/len(Z)\n",
    "    var = []\n",
    "    for i in range(Z.shape[1]):\n",
    "        sum1 = 0\n",
    "        for j in range(Z.shape[0]):\n",
    "            sum1 += (Z[j,i] - mean[i])**2\n",
    "        var.append(1. *sum1/len(Z))\n",
    "\n",
    "    for i in range(Z.shape[1]):\n",
    "        for j in range(Z.shape[0]):\n",
    "            Z[j,i] = (Z[j,i] - mean[i])/sqrt(var[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###function to predict the accuracy on the basis of the predicted value###\n",
    "\n",
    "def accuracy(Y,Y_hat):\n",
    "                \n",
    "    sum1 = sum([1 for i in range(len(Y)) if Y[i] == Y_hat[i]])\n",
    "    return (1.0*sum1/len(Y))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### getting and preparing the data##\n",
    "## we also apply PCA here to see if we can reduce the data furture#####\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "data = urllib2.urlopen(url)\n",
    "data = data.read()\n",
    "data = data.split('\\n')[:-1]\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for i in data:\n",
    "    \n",
    "    x = i.split(',')\n",
    "   \n",
    "    \n",
    "    Y.append(float(x[len(x)-1]))\n",
    "    x = x[0:len(x)-1]\n",
    "    #print x\n",
    "    for j in range(len(x)):\n",
    "        if x[j] == '?':\n",
    "            #print True\n",
    "            x[j] = -1\n",
    "    x = map(float,x)\n",
    "        \n",
    "    #print type(i)\n",
    "    X.append(x)\n",
    "    \n",
    "X = np.array(X)\n",
    "\n",
    "Y = np.reshape(np.array(Y),(len(Y),))\n",
    "Y[Y > 0] = 1 ###changing the lables to 0 and 1\n",
    "\n",
    "for i in range(X.shape[1]): #####imputing the missing values with mode of the feature\n",
    "    m =  mode(X[:,i])[0][0]\n",
    "    for j in range(X.shape[0]):\n",
    "        if X[j][i] == -1:\n",
    "            X[j][i] = m\n",
    "\n",
    "\n",
    "X_norm = norm(X.copy())##normalizing the data\n",
    "\n",
    "cov_matrix = np.dot(np.transpose(X_norm),X_norm)\n",
    "eigen= np.linalg.eig(cov_matrix)\n",
    "eigen_val = eigen[0]\n",
    "#print eigen_val\n",
    "eigen_vec = eigen[1]\n",
    "total_eigen_val = np.sum(eigen_val)\n",
    "sum_d = []\n",
    "for i in range(len(eigen_val)):\n",
    "    sum_d.append(1. *np.sum(eigen_val[i+1:])/total_eigen_val)\n",
    "width = 0.35\n",
    "plt.bar(range(len(eigen_val)),sum_d,width)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "W = eigen_vec[:,:]\n",
    "\n",
    "Z = []\n",
    "for i in X_norm:\n",
    "    Z.append(np.dot(np.transpose(W),i))\n",
    "Z = np.array(Z)\n",
    "\n",
    "print \"X's shape: \", X.shape\n",
    "print \"Z's shape: \", Z.shape\n",
    "print \"Y's shape: \", Y.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colnames = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs','restecg', 'thalach', 'exang','oldpeak','slope','ca','thal']\n",
    "df = pd.DataFrame(data=X,columns=colnames)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############exploratory data analysis#######################################\n",
    "\n",
    "numeric_idx =  [0,3,4,7,9]\n",
    "categorical_idx = set(range(0,13)) - set(numeric_idx)\n",
    "def exploratory_data_analysis(numerical,categorical):\n",
    "    #PLotting Box Plots for numerical attributes.\n",
    "    for i in numerical:\n",
    "        df.boxplot(column = colnames[i])\n",
    "        plt.show()\n",
    "    #Plotting Histograms to show the class distributions of the features.\n",
    "    for i in categorical:\n",
    "        df[colnames[i]].hist()\n",
    "        plt.show()\n",
    "\n",
    "exploratory_data_analysis(numeric_idx,categorical_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######ID3##############\n",
    "UNCLASSIFIED_VALUE = -99\n",
    "\n",
    "sample_mapping = namedtuple('TrainingSample', ('record', 'target'))\n",
    "\n",
    "\n",
    "class Id3(object):\n",
    "    #Initializing the Roote node to None.\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "     \n",
    "    #Method for Fitting the ID3 Classifier.\n",
    "    def build(self, rows, target):\n",
    "        training_recs = []\n",
    "        for i,j in zip(rows,target):\n",
    "            training_recs.append(sample_mapping(i,j))\n",
    "        \n",
    "        #Calling function to create the decision tree.\n",
    "        self.root = self.decision_tree(training_recs, features_pred = range(len(rows[0])))\n",
    "    #Method used for creation of the decision tree. This method would be used for the recursive creation of the Childrens \n",
    "    #(Decision tree) too.\n",
    "    \n",
    "    def most_common(self,rows):\n",
    "        classes = []\n",
    "        for i in rows:\n",
    "            classes.append(i.target)\n",
    "        most_common_class = Counter(classes).most_common()[0][0]\n",
    "        return most_common_class\n",
    "    \n",
    "    def decision_tree(self, rows, features_pred):\n",
    "        #Checking if all the features have been used for making the decision. Assign the classes which are most common at th\n",
    "        #parent node.\n",
    "        \n",
    "        if len(features_pred) == 0:\n",
    "            class_def = self.most_common(rows)\n",
    "            root = CreateLeaf(class_def) \n",
    "        else:\n",
    "            classes = []\n",
    "            for i in rows:\n",
    "                classes.append(i.target)\n",
    "            if len(set(classes)) == 1:\n",
    "                assign_target_class = rows[0].target\n",
    "                root = CreateLeaf(assign_target_class)\n",
    "            else:\n",
    "                split_attribute = self.select_best_attribute(rows,features_pred,classes)\n",
    "                root = CreateNode(split_attribute)\n",
    "                best_attr_val = set([i.record[split_attribute] for i in rows])\n",
    "                #Check for the vals for splitting.\n",
    "                for val in best_attr_val:\n",
    "                    rec = []\n",
    "                    for i in rows:\n",
    "                        if i.record[split_attribute] == val:\n",
    "                            rec.append(i)\n",
    "                    #create children nodes.\n",
    "                    child = self.decision_tree(rec,features_pred)\n",
    "                    root[val] = child\n",
    "        return root\n",
    "     \n",
    "    \n",
    "    def select_best_attribute(self, rows, features, classes):\n",
    "        ig = []\n",
    "        for i in features:\n",
    "            ig.append((self.info_gain_calc(rows, i, classes), i))\n",
    "        best_attr = sorted(ig, key=lambda x: x[1])[-1][1]\n",
    "        features.pop(features.index(best_attr))\n",
    "        return best_attr\n",
    "\n",
    "    def info_gain_calc(self, rows, feature, classes):\n",
    "        branches = defaultdict(list)\n",
    "        entropy_before = self.entropy(classes)\n",
    "        for i in rows:\n",
    "            branches[i.record[feature]].append(i)\n",
    "        after_branching_entropy = 0.0\n",
    "        for i in branches.values():\n",
    "            class_branch = []\n",
    "            for j in i:\n",
    "                class_branch.append(j.target)\n",
    "            after_branching_entropy += (len(branches) / len(rows)) * self.entropy(class_branch)\n",
    "        return entropy_before - after_branching_entropy\n",
    "    \n",
    "  \n",
    "    def entropy(self,data):\n",
    "        def log2(x):\n",
    "            if x == 0:\n",
    "                return 0\n",
    "            else:\n",
    "                return log(x)/log(2)\n",
    "        counter = Counter(data)\n",
    "        N = len(data)\n",
    "        ent = []\n",
    "        for i in counter:\n",
    "            ent.append(-1.0*(counter[i] / len(data))*log2(counter[i] / len(data)))\n",
    "        return sum(ent)\n",
    "    \n",
    "    def predict(self, X, allow_unclassified=False):\n",
    "        class_def = 1\n",
    "        if allow_unclassified:\n",
    "            class_def = UNCLASSIFIED_VALUE\n",
    "        predicted = []\n",
    "\n",
    "        for sample in X:\n",
    "            clas = None\n",
    "            current_node = self.root\n",
    "            while clas is None:\n",
    "                if current_node.is_leaf():\n",
    "                    clas = current_node.clas\n",
    "                else:\n",
    "                    \n",
    "                    key_value = sample[current_node.feature]\n",
    "                    if key_value in current_node:\n",
    "                        current_node = current_node[key_value]\n",
    "                    else:\n",
    "                        clas = class_def\n",
    "            predicted.append(clas)\n",
    "        return predicted\n",
    "   \n",
    "    \n",
    "\n",
    "class CreateNode(dict):\n",
    "    #instances = []\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature\n",
    "        #CreateNode.instances.append(self) #Added for keeping track the instances\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return False\n",
    "\n",
    "\n",
    "class CreateLeaf(dict):\n",
    "    #inst = []\n",
    "    def __init__(self, clas):\n",
    "        #CreateLeaf.inst.append(self) #Added for keeping track the instances\n",
    "        self.clas = clas\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return True\n",
    "clf = Id3()\n",
    "clf.build(X_norm[:250],Y[:250])\n",
    "Y_hat_id3 = clf.predict(X_norm[250:])\n",
    "\n",
    "#print \"Accuracy of the ID3 algorithm is: %s\" %(accuracy(Y[250:],Y_hat_id3))\n",
    "print \"Accuracy of the Decision Tree is: %s\" %(accuracy(Y[250:],Y_hat_id3))\n",
    "print \"Below are the various statistics for the Decision Tree\"\n",
    "print \"=======================================================\"\n",
    "print metrics.classification_report(Y[250:],Y_hat_id3)\n",
    "print \"Confusion Matrix\"\n",
    "print \"================\"\n",
    "plot_confusion_matrix(Y[250:],Y_hat_id3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################gaussian SVM##########################\n",
    "\n",
    "\n",
    "def prediction_gaussian(X,X_SV,Y_SV,W0,alpha_SV):\n",
    "    \n",
    "    Y_hat = []\n",
    "    \n",
    "    for x in X:\n",
    "        \n",
    "        sum1 = 0\n",
    "        \n",
    "        for i in range(len(alpha_SV)):\n",
    "            \n",
    "            sum1 += alpha_SV[i]*Y_SV[i]*gaussian_similarity(X_SV[i],x)\n",
    "        \n",
    "        if sum1 + W0 > 0:\n",
    "            \n",
    "            \n",
    "            Y_hat.append(1)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            Y_hat.append(-1)\n",
    "            \n",
    "    return np.array(Y_hat)   \n",
    "\n",
    "def gaussian_similarity(X,Y,sigma = 3.0):\n",
    "    #m = np.dot(np.transpose(X-Y),(X-Y))\n",
    "    return np.exp((-(np.linalg.norm(X-Y)**2))/(2*sigma**2))\n",
    "\n",
    "\n",
    "def gram_matrix_gaussian(X):\n",
    "    m = len(X)\n",
    "    gram = np.zeros(shape=(m,m))\n",
    "    for i in range(m):\n",
    "        x = X[i]\n",
    "        for j in range(m):        \n",
    "            y = X[j]\n",
    "            gram[i][j] += gaussian_similarity(x,y)\n",
    "    \n",
    "    return gram\n",
    "\n",
    "\n",
    "def training_gaussian(X,Y,e,c):\n",
    "    \n",
    "    P = (gram_matrix_gaussian(X)) * (np.dot(Y,np.transpose(Y)))\n",
    "    #print P\n",
    "    q = np.zeros(shape=(X.shape[0],1))\n",
    "    q.fill(-1)\n",
    "    \n",
    "    G1 = np.zeros(shape=(X.shape[0],X.shape[0]))\n",
    "    np.fill_diagonal(G1,-1)\n",
    "    \n",
    "    G2 = np.zeros(shape=(X.shape[0],X.shape[0]))\n",
    "    np.fill_diagonal(G2,1)\n",
    "    \n",
    "    G = np.concatenate((G1,G2))\n",
    "    \n",
    "    H1 = np.zeros(shape = (X.shape[0],1))\n",
    "    H2 = np.full((X.shape[0],1),c)\n",
    "    H = np.concatenate((H1,H2))\n",
    "    \n",
    "    A = np.transpose(Y)\n",
    "    \n",
    "    b = 0\n",
    "    \n",
    "    soln = solvers.qp(matrix(P,tc = 'd'),\n",
    "                  matrix(q,tc = 'd'),\n",
    "                  matrix(G,tc = 'd'),\n",
    "                  matrix(H,tc = 'd'),\n",
    "                  matrix(A,tc = 'd'),\n",
    "                  matrix(b,tc='d'))\n",
    "    \n",
    "    alpha = (np.ravel(soln['x']))\n",
    "    #alpha = np.reshape(np.array(alpha),(len(alpha),1))\n",
    "    \n",
    "    #print alpha\n",
    "     \n",
    "    alpha_SV = alpha[alpha>e]\n",
    "    X_SV = X[alpha>e]\n",
    "    Y_SV = Y[alpha>e]\n",
    "    index = np.where(alpha>e)\n",
    "    \n",
    "    #print alpha_SV\n",
    "    #print X_SV\n",
    "    #print Y_SV\n",
    "    #print index[0]\n",
    "    \n",
    "    #print len(index[0])\n",
    "    \n",
    "    W0 = 0\n",
    "    for i in range(len(alpha_SV)):\n",
    "        sum1 = 0\n",
    "        for j in range(len(alpha_SV)):\n",
    "            sum1 += alpha_SV[j]*Y_SV[j]*gaussian_similarity(X[j],X[i])\n",
    "        W0 += Y[i] - sum1\n",
    "    \n",
    "    W0 = 1. * W0/len(alpha_SV) \n",
    "    \n",
    "    #print W0\n",
    "    \n",
    "    \n",
    "    \n",
    "    return (alpha_SV,X_SV,Y_SV,W0,index[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_SVM(X,Y,e,c):\n",
    "    Y = np.reshape(np.array(Y),(len(Y),1))\n",
    "    Y[Y == 0] = -1\n",
    "    alpha_SV,X_SV,Y_SV,W0,index = training_gaussian(X[:250],Y[:250],e,c)\n",
    "    #plotting_SVs(X[:0.75 * len(X)],Y[:0.75 * len(X)],index)\n",
    "    Y_hat_svm = prediction_gaussian(X[250:],X_SV,Y_SV,W0,alpha_SV)\n",
    "    #print Y_hat\n",
    "    print accuracy(Y[250:],Y_hat_svm)\n",
    "    Y_hat_svm[Y_hat_svm == -1] = 0\n",
    "    return Y_hat_svm\n",
    "\n",
    "\n",
    "\n",
    "Y_hat_svm = gaussian_SVM(X_norm,Y,0.0001,5)\n",
    "\n",
    "\n",
    "print \"Accuracy of the SVM Model is: %s\" %(accuracy(Y[250:],Y_hat_svm))\n",
    "print \"Below are the various statistics for the SVM Model\"\n",
    "print \"==================================================\"\n",
    "print metrics.classification_report(Y[250:],Y_hat_svm)\n",
    "print \"Confusion Matrix\"\n",
    "print \"================\"\n",
    "\n",
    "plot_confusion_matrix(Y[250:],Y_hat_svm) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################### NUERAL NETWORK#######################\n",
    "\n",
    "\n",
    "def indicator(y,cls):\n",
    "    if y==cls:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def softmax(V,z,k):\n",
    "    \n",
    "    \n",
    "    #print z.shape\n",
    "    d = 0\n",
    "    for v in V:\n",
    "        \n",
    "        #print v.shape\n",
    "        \n",
    "        d += exp(np.dot(np.transpose(v),z))\n",
    "    n = exp(np.dot(np.transpose(V[k]),z))\n",
    "    return (1.*n/d)\n",
    "\n",
    "\n",
    "def sigmoid(w,x):\n",
    "    \n",
    "    return (1/(1 + exp(-np.dot(np.transpose(w),x))))\n",
    "\n",
    "def compute_Z_Y(X,W,V):\n",
    "    \n",
    "    Z = []\n",
    "    \n",
    "    Y_hat = []\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        \n",
    "        z = [1.0]\n",
    "        \n",
    "        y = []\n",
    "        \n",
    "        for w in W:\n",
    "        \n",
    "            z.append(sigmoid(w,X[i]))\n",
    "        \n",
    "        Z.append(z)          \n",
    "            \n",
    "        for k in range(V.shape[0]):\n",
    "            \n",
    "            #print k\n",
    "            \n",
    "            y.append(softmax(V,z,k))\n",
    "            \n",
    "            #print y\n",
    "        \n",
    "        Y_hat.append(y)\n",
    "        \n",
    "    #print Z\n",
    "    #print Y_hat\n",
    "              \n",
    "    return np.array(Z),np.array(Y_hat)\n",
    "\n",
    "def likelihood(X,Y,K,Y_hat):\n",
    "    \n",
    "    l = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        for k in range(K):\n",
    "            l += indicator(Y[i],k) * (log10(Y_hat[i][k]))\n",
    "    l = - l\n",
    "    return l\n",
    "\n",
    "def compute_Y_pred(Y_hat):\n",
    "    Y_pred = []\n",
    "    for y_hat in Y_hat:\n",
    "        #print y_hat\n",
    "        \n",
    "        Y_pred.append(np.argmax(y_hat))\n",
    "            \n",
    "    return np.reshape(np.array(Y_pred),(len(Y_pred),1))\n",
    "\n",
    "def training(X,Y,cat,H,lr):\n",
    "    \n",
    "    W = np.zeros(shape=(H,X.shape[1]))\n",
    "    \n",
    "    W.fill(0.00)\n",
    "    \n",
    "    K = len(cat)\n",
    "    \n",
    "    V = np.zeros(shape=(K,H+1))\n",
    "    \n",
    "    V.fill(0.00)\n",
    "    \n",
    "    Z,Y_hat = compute_Z_Y(X,W,V)\n",
    "    \n",
    "    l_new = likelihood(X,Y,K,Y_hat)\n",
    "    \n",
    "    it = 0\n",
    "    \n",
    "    best = False\n",
    "    \n",
    "    while best == False: \n",
    "        \n",
    "        l_pre = l_new\n",
    "        \n",
    "    \n",
    "        for k in range(K):\n",
    "\n",
    "            grad_V = 0\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "\n",
    "                grad_V += (Y_hat[i][k] - indicator(Y[i],k))* Z[i]\n",
    "\n",
    "            #print grad_V\n",
    "\n",
    "            V[k] = V[k] - (lr*grad_V)\n",
    "\n",
    "\n",
    "\n",
    "        for h in range(H):\n",
    "\n",
    "            grad_W = 0\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "\n",
    "                x = 0\n",
    "\n",
    "                for k in range(K):\n",
    "\n",
    "                    x += (Y_hat[i][k] - indicator(Y[i],k)) * V[k][h+1]\n",
    "\n",
    "\n",
    "                    grad_W +=  x * Z[i][h+1] * (1.0 - Z[i][h+1]) * X[i]\n",
    "\n",
    "                W[h] = W[h] - lr*grad_W\n",
    "\n",
    "\n",
    "        Z,Y_hat = compute_Z_Y(X,W,V)\n",
    "\n",
    "        it += 1\n",
    "\n",
    "        l_new = likelihood(X,Y,K,Y_hat)\n",
    "        \n",
    "        print l_new\n",
    "        \n",
    "        if it==100 or abs(l_new-l_pre) < 0.01:\n",
    "            \n",
    "            best = True\n",
    "        \n",
    "        \n",
    "    return V,W\n",
    "\n",
    "cat = (set(Y))\n",
    "V,W = training(X_norm[:250],Y[:250],cat,50,0.001)\n",
    "\n",
    "\n",
    "Z_test,Y_hat = compute_Z_Y(X_norm[250:],W,V)\n",
    "\n",
    "\n",
    "Y_hat_nn = compute_Y_pred(Y_hat)\n",
    "\n",
    "print \"testing error\", accuracy(Y[250:],Y_hat_nn)\n",
    "\n",
    "\n",
    "print \"Accuracy of the Neural Network Model is: %s\" %(accuracy(Y[250:],Y_hat_nn))\n",
    "print \"Below are the various statistics for the Neural Network Model\"\n",
    "print \"==================================================\"\n",
    "print metrics.classification_report(Y[250:],Y_hat_nn)\n",
    "print \"Confusion Matrix\"\n",
    "print \"================\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(Y[250:],Y_hat_nn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######Bag boosting approach#############\n",
    "\n",
    "Y_pred = []\n",
    "for i in range(len(Y_hat_id3)):\n",
    "    m = mode([Y_hat_id3[i],Y_hat_nn[i],Y_hat_svm[i]])\n",
    "    #print m\n",
    "    Y_pred.append(m[0][0])\n",
    "print \"Accuracy of the Bagging approach is: %s\" %(accuracy(Y[250:],Y_pred))\n",
    "print \"Below are the various statistics for the Ensemble model\"\n",
    "print \"=======================================================\"\n",
    "print metrics.classification_report(Y[250:],Y_pred)\n",
    "print \"Confusion Matrix\"\n",
    "print \"================\"\n",
    "plot_confusion_matrix(Y[250:],Y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
